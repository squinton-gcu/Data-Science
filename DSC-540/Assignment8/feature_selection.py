# -*- coding: utf-8 -*-
"""Feature selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hbk_LEeu2I5CKYtEeBWucH3p_jS3AlLw
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip install pandas

!pip install numpy

# programmer - Sophia Quinton
# date - 12-22-21
# class - DSC -540
# assignment - Assignment 8

#acquire data
#(McKinley et. al., 2010) (Harris et. al., 2020)
import pandas as pd
import numpy as np

#(Urban, 2019)
def make_views(
  arr,
  win_size,
  step_size,
  writeable = False,
):
  """
  arr: any 2D array whose columns are distinct variables and 
    rows are data records at some timestamp t
  win_size: size of data window (given in data points along record/time axis)
  step_size: size of window step (given in data point along record/time axis)
  writable: if True, elements can be modified in new data structure, which will affect
    original array (defaults to False)
  
  Note that step_size is related to window overlap (overlap = win_size - step_size), in 
  case you think in overlaps.
  """
  
  n_records = arr.shape[0]
  n_columns = arr.shape[1]
  remainder = (n_records - win_size) % step_size 
  num_windows = 1 + int((n_records - win_size - remainder) / step_size)
  new_view_structure = np.lib.stride_tricks.as_strided(
    arr,
    shape = (num_windows, win_size, n_columns),
    strides = (8 * step_size * n_columns, 8 * n_columns, 8),
    writeable = False,
  )
  return new_view_structure

#(Chowdhury et. al., 2017)
def spectral(input_column):
  FFT = np.fft.fft(input_column)
  FFT_max = float(max(FFT))
  summation_data =  np.sum(np.square(np.absolute(FFT)))
  final_result = summation_data/1000
  return final_result, FFT_max

def zeroCross(input):
  ##(Chowdhury et. al., 2017) (Muller, 2015)
  X = input-np.median(input)
  result = ((X[:-1] * X[1:]) < 0).sum()
  return result

#s_1_cut2
def get_activity(index, full_frame):
  if index != 0:
    new_index = index + 1000
  else:
    new_index = index
  act = np.array(full_frame[1])[new_index]
  return act

def get_correlate(table_frame, index1, index2, index3):
  k =pd.DataFrame({'x':table_frame[index1][index2], 'y':table_frame[index1][index3]})
  correlate = k.corr()
  out = correlate['x'][1]
  return out

##get metrics
from scipy.stats import kurtosis, skew
def get_metrics(table_frame, full_frame):
  mean = []
  standard = []
  minimum = []
  maximum = []
  variance = []
  median = []
  skewness = []
  p_25 = []
  p_75 = []
  kurt = []
  spectral_e = []
  cor_axis = []
  zero_cross = []
  dominantF = []
  activity = []
  counter = 0

  for i in range(len(table_frame)):
      mean_group = []
      standard_group = []
      min_group = []
      max_group = []
      var_group = []
      median_group = []
      skewness_group = []
      p25_group = []
      p75_group = []
      kurtosis_group = []
      spectral_e_group = []
      zero_cross_group = []
      dominantF_group = []

      for j in range(3):
          column = [ row[j] for row in table_frame[i]]
          mean_1 = np.mean(column)
          standard_1 = np.std(column)
          min_1 = min(column)
          max_1 = max(column)
          var_1 = np.var(column)
          median_1 = np.median(column)
          skewness_1 = skew(column)
          p25_1 = np.percentile(column, 25)
          p75_1 = np.percentile(column, 75)
          kurtosis_1 = kurtosis(column)
          spectral_e_1, max_FFT_1 = spectral(column)
          zero_cross_1 = zeroCross(column)

          mean_group.append(mean_1)
          standard_group.append(standard_1)
          min_group.append(min_1)
          max_group.append(max_1)
          var_group.append(var_1)
          median_group.append(median_1)
          skewness_group.append(skewness_1)
          p25_group.append(p25_1)
          p75_group.append(p75_1)
          kurtosis_group.append(kurtosis_1)
          spectral_e_group.append(spectral_e_1)
          zero_cross_group.append(zero_cross_1)
          dominantF_group.append(max_FFT_1)

      mean.append(mean_group)
      standard.append(standard_group)
      minimum.append(min_group)
      maximum.append(max_group)
      variance.append(var_group)
      median.append(median_group)
      skewness.append(skewness_group)
      p_25.append(p25_group)
      p_75.append(p75_group)
      kurt.append(kurtosis_group)
      spectral_e.append(spectral_e_group)
      zero_cross.append(zero_cross_group)
      dominantF.append(dominantF_group)

      #correlation tests
      xy = get_correlate(table_frame, i, 0, 1)
      xz = get_correlate(table_frame, i, 0, 2)
      yz = get_correlate(table_frame, i, 1, 2)
      correlation_group = np.array([xy, xz, yz])
      cor_axis.append(correlation_group)

      #activity
      activity_1 = get_activity(counter, full_frame)
      activity.append(activity_1)
      counter += 1

  sub1 = pd.concat([pd.DataFrame(mean),
           pd.DataFrame(standard),
           pd.DataFrame(minimum),
           pd.DataFrame(maximum),
           pd.DataFrame(variance),
           pd.DataFrame(median),
           pd.DataFrame(skewness),
           pd.DataFrame(p_25),
           pd.DataFrame(p_75),
           pd.DataFrame(kurt),
           pd.DataFrame(spectral_e),
           pd.DataFrame(zero_cross),
           pd.DataFrame(dominantF),
           pd.DataFrame(cor_axis),
           pd.DataFrame(activity)], axis = 1)
  return sub1

#combine the results
def combine_results(current_frame, frame_to_add):
  frame = pd.concat([current_frame, frame_to_add], axis=0)
  return frame

win = 1000
steps = 500
def all_subjects(path, window_size, step, full_frame):
  s_1 = pd.read_table(path, sep="\s+", header=None)
  s_1_cut = s_1.iloc[:,[0,1,5,6,7]]
  s_1_cut = s_1_cut[s_1_cut[1] < 9]
  #(Agrawal, 2021)
  s_1_cut.interpolate()

  max_1 = max(s_1_cut[0])
  min_1 = min(s_1_cut[0])
  s_1_cut2 = s_1_cut[s_1_cut[0] < (10-max_1)]
  s_1_cut2 = s_1_cut[s_1_cut[0] > (10+min_1)]
  s_1_cut2 = s_1_cut2.set_index(0)
  s_1_cut22 = s_1_cut2[[5,6,7]]
  array_s1 = np.array(s_1_cut22)

  test1 = make_views(array_s1, window_size, step)
  new_frame = get_metrics(test1, s_1_cut2)
  next_frame = combine_results(full_frame, new_frame)
  return next_frame

full_frame = pd.DataFrame()
s_1 = all_subjects("/content/gdrive/MyDrive/data/subject101.dat", win, steps, full_frame)
s_2 = all_subjects("/content/gdrive/MyDrive/data/subject102.dat", win, steps, s_1)
s_3 = all_subjects("/content/gdrive/MyDrive/data/subject103.dat", win, steps, s_2)
s_4 = all_subjects("/content/gdrive/MyDrive/data/subject104.dat", win, steps, s_3)
s_5 = all_subjects("/content/gdrive/MyDrive/data/subject105.dat", win, steps, s_4)
s_6 = all_subjects("/content/gdrive/MyDrive/data/subject106.dat", win, steps, s_5)
s_7 = all_subjects("/content/gdrive/MyDrive/data/subject107.dat", win, steps, s_6)
s_8 = all_subjects("/content/gdrive/MyDrive/data/subject108.dat", win, steps, s_7)
s_9 = all_subjects("/content/gdrive/MyDrive/data/subject109.dat", win, steps, s_8)

s_9.columns = ["meanx", "meany", "meanz",
               "standardx", "standardy", "standardz",
               "minimumx", "minimumy", "minimumz",
               "maximumx", "maximumy", "maximumz",
               "variancex", "variancey", "variancez",
               "medianx", "mediany", "medianz",
               "skewnessx", "skewnessy", "skewnessz",
               "percent_25x", "percent_25y", "percent_25z",
               "percent_75x", "percent_75y", "percent_75z",
               "kurtosisx", "kurtosisy", "kurtosisz",
               "spectralE_x", "spectralE_y", "spectralE_z",
               "zero_crossx", "zero_crossy", "zero_crossz",
               "dominant_freqx", "dominant_freqy", "dominant_freqz",
               "corxy", "corxz", "coryz",
               "activity"]

s_9.to_csv("/content/gdrive/MyDrive/data/feature_extract.csv")

s_1 = pd.read_table("/content/gdrive/MyDrive/data/subject101.dat", sep="\s+", header=None)
s_1_cut = s_1.iloc[:,[0,1,2,5,6,7]]
s_1_cut = s_1_cut[s_1_cut[1] < 9]

s_1_cut[1][5000]

get_activity(5000, s_1_cut)

s_9

s_2

